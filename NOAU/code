import torch
import torch.nn as nn
import torch.nn.functional as F
from torchdiffeq import odeint  # pip install torchdiffeq

# -----------------------------------------------------------------------------
Neural ODE-based Adaptive Upsampling
# 1. Neural ODE Function for Upsampling
# This module defines the ODE dynamics over a “time” dimension.
# It is applied to a feature map at each pixel location.
# -----------------------------------------------------------------------------
class ODEFunc(nn.Module):
    def __init__(self, channels):
        super(ODEFunc, self).__init__()
        # A simple residual block that models dF/dt = f(F,t)
        self.net = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        )
    def forward(self, t, x):
        return self.net(x)

# -----------------------------------------------------------------------------
# 2. Neural ODE Upsampling Branch
# This branch evolves the low-resolution feature map from time t0 to t1.
# The evolution “upsamples” the spatial resolution via learned dynamics.
# -----------------------------------------------------------------------------
class NeuralODEUpsample(nn.Module):
    def __init__(self, channels, t0=0.0, t1=1.0):
        super(NeuralODEUpsample, self).__init__()
        self.odefunc = ODEFunc(channels)
        self.t0 = t0
        self.t1 = t1

    def forward(self, x):
        """
        x: [B, C, H, W] low-res feature map.
        We interpret x as the initial condition at time t0.
        The neural ODE solver integrates from t0 to t1.
        """
        # Solve ODE; odeint returns tensor of shape [num_t, B, C, H, W]
        t = torch.tensor([self.t0, self.t1], device=x.device, dtype=x.dtype)
        x_t = odeint(self.odefunc, x, t, method='rk4')  # using Runge-Kutta 4th order
        x_out = x_t[-1]  # final state at t1
        return x_out

# -----------------------------------------------------------------------------
# 3. Convolutional Upsampling Branch
# A standard branch to upsample features using interpolation + convolution.
# -----------------------------------------------------------------------------
class ConvUpsample(nn.Module):
    def __init__(self, in_channels, scale_factor=2):
        super(ConvUpsample, self).__init__()
        self.scale_factor = scale_factor
        # Here we use a simple pixel-shuffle–like approach: first expand channels, then pixel shuffle.
        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), kernel_size=3, padding=1)
        self.pixel_shuffle = nn.PixelShuffle(scale_factor)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # x: [B, C, H, W]
        x = self.conv(x)  # [B, C * sf^2, H, W]
        x = self.pixel_shuffle(x)  # [B, C, H*sf, W*sf]
        x = self.relu(x)
        return x

# -----------------------------------------------------------------------------
# 4. Attention-based Fusion Module
# Fuses the outputs of the Neural ODE branch and the convolution branch.
# -----------------------------------------------------------------------------
class FusionModule(nn.Module):
    def __init__(self, channels):
        super(FusionModule, self).__init__()
        # Compute an attention mask from the concatenated features.
        self.attn_conv = nn.Sequential(
            nn.Conv2d(channels * 2, channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, 2, kernel_size=1)  # two attention maps: one per branch
        )
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, ode_out, conv_out):
        # Both inputs assumed to be [B, C, H, W]
        fusion_input = torch.cat([ode_out, conv_out], dim=1)
        attn = self.attn_conv(fusion_input)  # [B, 2, H, W]
        attn = self.softmax(attn)  # attention weights sum to 1 over branch dimension
        attn_ode, attn_conv = attn[:, 0:1, :, :], attn[:, 1:2, :, :]
        # Weighted sum of the two branches
        fused = attn_ode * ode_out + attn_conv * conv_out
        return fused

# -----------------------------------------------------------------------------
# 5. Full NODA-U Network
# The network uses a shared encoder, then feeds features into two upsampling branches,
# and finally fuses their outputs via the attention-based fusion module.
# Optionally, a final refinement layer produces the output image.
# -----------------------------------------------------------------------------
class NODAUpsampler(nn.Module):
    def __init__(self, base_channels=64, scale_factor=2):
        super(NODAUpsampler, self).__init__()
        self.scale_factor = scale_factor

        # Simple encoder to extract low-res features from input image.
        self.encoder = nn.Sequential(
            nn.Conv2d(3, base_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

        # Two upsampling branches:
        self.neural_ode_branch = NeuralODEUpsample(base_channels)
        self.conv_branch = ConvUpsample(base_channels, scale_factor=scale_factor)

        # Upsample features from the neural ODE branch using interpolation to match conv branch resolution.
        self.feature_upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)

        # Fusion module to merge the two branch outputs.
        self.fusion = FusionModule(base_channels)

        # A final refinement convolution to produce output image.
        self.refine = nn.Sequential(
            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels, 3, kernel_size=1)
        )

    def forward(self, x):
        """
        x: [B, 3, H, W] low-resolution input image.
        """
        # 1. Encode low-res features
        feat = self.encoder(x)  # [B, base_channels, H, W]

        # 2. Neural ODE branch: evolve features continuously.
        ode_feat = self.neural_ode_branch(feat)  # [B, base_channels, H, W]
        # Upsample ode features by interpolation to get same spatial size as conv branch output.
        ode_up = self.feature_upsample(ode_feat)  # [B, base_channels, H*sf, W*sf]

        # 3. Convolutional branch: standard upsampling.
        conv_up = self.conv_branch(feat)  # [B, base_channels, H*sf, W*sf]

        # 4. Fusion: merge the two outputs adaptively.
        fused = self.fusion(ode_up, conv_up)  # [B, base_channels, H*sf, W*sf]

        # 5. Refinement: output final high-res image.
        out = self.refine(fused)  # [B, 3, H*sf, W*sf]
        return out

# -----------------------------------------------------------------------------
# 6. Example Training Step
# Demonstrate a forward pass, compute losses, and run a backward pass.
# -----------------------------------------------------------------------------
if __name__ == '__main__':
    # Create dummy data: low-res input and high-res target.
    B = 2
    H, W = 32, 32
    scale = 2
    x = torch.randn(B, 3, H, W)
    # For demonstration, simply upsample the input via bilinear interpolation to serve as a synthetic target.
    target = F.interpolate(x, scale_factor=scale, mode='bilinear', align_corners=True)

    model = NODAUpsampler(base_channels=64, scale_factor=scale)
    out = model(x)
    print("Output shape:", out.shape)  # Expected [B, 3, H*scale, W*scale]

    # Define a simple loss: L1 pixel loss plus a frequency domain loss.
    l1_loss = nn.L1Loss()
    loss_pixel = l1_loss(out, target)
    
    # Frequency loss: compare FFT magnitudes
    fft_out = torch.abs(torch.fft.fft2(out))
    fft_target = torch.abs(torch.fft.fft2(target))
    loss_freq = l1_loss(fft_out, fft_target)
    
    total_loss = loss_pixel + 0.1 * loss_freq
    print("Total Loss:", total_loss.item())

    total_loss.backward()
